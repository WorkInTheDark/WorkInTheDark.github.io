---
---

@article{xu2023leveraging,
  title={Leveraging Large Language Models for Mental Health Prediction via Online Text Data},
  author={Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Yu, Hong and Hendler, James and Dey, Anind K and Wang, Dakuo},
  journal={arXiv preprint arXiv:2307.14385},
  year={2023}
}

@article{yao2023beyond,
  title={Beyond Labels: Empowering Human with Natural Language Explanations through a Novel Active-Learning Architecture},
  author={Yao, Bingsheng and Jindal, Ishan and Popa, Lucian and Katsis, Yannis and Ghosh, Sayan and He, Lihong and Lu, Yuxuan and Srivastava, Shashank and Hendler, James and Wang, Dakuo},
  journal={arXiv preprint arXiv:2305.12710},
  year={2023}
}

@article{yao2023human,
  title={Are human explanations always helpful? towards objective evaluation of human natural language explanations},
  author={Yao, Bingsheng and Sen, Prithviraj and Popa, Lucian and Hendler, James and Wang, Dakuo},
  journal={arXiv preprint arXiv:2305.03117},
  year={2023}
}

@article{xu2022nece,
  title={Nece: Narrative event chain extraction toolkit},
  author={Xu, Guangxuan and Isaza, Paulina Toro and Li, Moshi and Oloko, Akintoye and Yao, Bingsheng and Sanctos, Cassia and Adebiyi, Aminat and Hou, Yufang and Peng, Nanyun and Wang, Dakuo},
  journal={arXiv preprint arXiv:2208.08063},
  year={2022}
}

@inproceedings{gehrmann-etal-2022-gemv2,
    title = "{GEM}v2: Multilingual {NLG} Benchmarking in a Single Line of Code",
    author = "Gehrmann, Sebastian  and
      Bhattacharjee, Abhik  and
      Mahendiran, Abinaya  and
      Wang, Alex  and
      Papangelis, Alexandros  and
      Madaan, Aman  and
      Mcmillan-major, Angelina  and
      Shvets, Anna  and
      Upadhyay, Ashish  and
      Bohnet, Bernd  and
      Yao, Bingsheng  and
      Wilie, Bryan  and
      Bhagavatula, Chandra  and
      You, Chaobin  and
      Thomson, Craig  and
      Garbacea, Cristina  and
      Wang, Dakuo  and
      Deutsch, Daniel  and
      Xiong, Deyi  and
      Jin, Di  and
      Gkatzia, Dimitra  and
      Radev, Dragomir  and
      Clark, Elizabeth  and
      Durmus, Esin  and
      Ladhak, Faisal  and
      Ginter, Filip  and
      Winata, Genta Indra  and
      Strobelt, Hendrik  and
      Hayashi, Hiroaki  and
      Novikova, Jekaterina  and
      Kanerva, Jenna  and
      Chim, Jenny  and
      Zhou, Jiawei  and
      Clive, Jordan  and
      Maynez, Joshua  and
      Sedoc, Jo{\~a}o  and
      Juraska, Juraj  and
      Dhole, Kaustubh  and
      Chandu, Khyathi Raghavi  and
      Beltrachini, Laura Perez  and
      Ribeiro, Leonardo F . R.  and
      Tunstall, Lewis  and
      Zhang, Li  and
      Pushkarna, Mahim  and
      Creutz, Mathias  and
      White, Michael  and
      Kale, Mihir Sanjay  and
      Eddine, Moussa Kamal  and
      Daheim, Nico  and
      Subramani, Nishant  and
      Dusek, Ondrej  and
      Liang, Paul Pu  and
      Ammanamanchi, Pawan Sasanka  and
      Zhu, Qi  and
      Puduppully, Ratish  and
      Kriz, Reno  and
      Shahriyar, Rifat  and
      Cardenas, Ronald  and
      Mahamood, Saad  and
      Osei, Salomey  and
      Cahyawijaya, Samuel  and
      {\v{S}}tajner, Sanja  and
      Montella, Sebastien  and
      Jolly, Shailza  and
      Mille, Simon  and
      Hasan, Tahmid  and
      Shen, Tianhao  and
      Adewumi, Tosin  and
      Raunak, Vikas  and
      Raheja, Vipul  and
      Nikolaev, Vitaly  and
      Tsai, Vivian  and
      Jernite, Yacine  and
      Xu, Ying  and
      Sang, Yisi  and
      Liu, Yixin  and
      Hou, Yufang",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-demos.27",
    doi = "10.18653/v1/2022.emnlp-demos.27",
    pages = "266--281",
    abstract = "Evaluations in machine learning rarely use the latest metrics, datasets, or human evaluation in favor of remaining compatible with prior work. The compatibility, often facilitated through leaderboards, thus leads to outdated but standardized evaluation practices. We pose that the standardization is taking place in the wrong spot. Evaluation infrastructure should enable researchers to use the latest methods and what should be standardized instead is how to incorporate these new evaluation advances.We introduce GEMv2, the new version of the Generation, Evaluation, and Metrics Benchmark which uses a modular infrastructure for dataset, model, and metric developers to benefit from each other{'}s work. GEMv2 supports 40 documented datasets in 51 languages, ongoing online evaluation for all datasets, and our interactive tools make it easier to add new datasets to the living benchmark.",
}

@inproceedings{yao-etal-2022-corpus,
    title = "A Corpus for Commonsense Inference in Story Cloze Test",
    author = "Yao, Bingsheng  and
      Joseph, Ethan  and
      Lioanag, Julian  and
      Si, Mei",
    booktitle = "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
    month = jun,
    year = "2022",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2022.lrec-1.375",
    pages = "3500--3508",
    abstract = "The Story Cloze Test (SCT) is designed for training and evaluating machine learning algorithms for narrative understanding and inferences. The SOTA models can achieve over 90{\%} accuracy on predicting the last sentence. However, it has been shown that high accuracy can be achieved by merely using surface-level features. We suspect these models may not \textit{truly} understand the story. Based on the SCT dataset, we constructed a human-labeled and human-verified commonsense knowledge inference dataset. Given the first four sentences of a story, we asked crowd-source workers to choose from four types of narrative inference for deciding the ending sentence and which sentence contributes most to the inference. We accumulated data on 1871 stories, and three human workers labeled each story. Analysis of the intra-category and inter-category agreements show a high level of consensus. We present two new tasks for predicting the narrative inference categories and contributing sentences. Our results show that transformer-based models can reach SOTA performance on the original SCT task using transfer learning but don{'}t perform well on these new and more challenging tasks.",
}

@inproceedings{10.1145/3491102.3517479,
author = {Zhang, Zheng and Xu, Ying and Wang, Yanhao and Yao, Bingsheng and Ritchie, Daniel and Wu, Tongshuang and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
title = {StoryBuddy: A Human-AI Collaborative Chatbot for Parent-Child Interactive Storytelling with Flexible Parental Involvement},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517479},
doi = {10.1145/3491102.3517479},
abstract = {Despite its benefits for children’s skill development and parent-child bonding, many parents do not often engage in interactive storytelling by having story-related dialogues with their child due to limited availability or challenges in coming up with appropriate questions. While recent advances made AI generation of questions from stories possible, the fully-automated approach excludes parent involvement, disregards educational goals, and underoptimizes for child engagement. Informed by need-finding interviews and participatory design (PD) results, we developed StoryBuddy, an AI-enabled system for parents to create interactive storytelling experiences. StoryBuddy’s design highlighted the need for accommodating dynamic user needs between the desire for parent involvement and parent-child bonding and the goal of minimizing parent intervention when busy. The PD revealed varied assessment and educational goals of parents, which StoryBuddy addressed by supporting configuring question types and tracking child progress. A user study validated StoryBuddy’s usability and suggested design insights for future parent-AI collaboration systems.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {218},
numpages = {21},
keywords = {child-agent interactions, interactive storytelling, human-AI collaboration, co-reading, voice user interfaces, dialogic reading},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{xu-etal-2022-fantastic,
    title = "Fantastic Questions and Where to Find Them: {F}airytale{QA} {--} An Authentic Dataset for Narrative Comprehension",
    author = "Xu, Ying  and
      Wang, Dakuo  and
      Yu, Mo  and
      Ritchie, Daniel  and
      Yao, Bingsheng  and
      Wu, Tongshuang  and
      Zhang, Zheng  and
      Li, Toby  and
      Bradford, Nora  and
      Sun, Branda  and
      Hoang, Tran  and
      Sang, Yisi  and
      Hou, Yufang  and
      Ma, Xiaojuan  and
      Yang, Diyi  and
      Peng, Nanyun  and
      Yu, Zhou  and
      Warschauer, Mark",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.34",
    doi = "10.18653/v1/2022.acl-long.34",
    pages = "447--460",
    abstract = "Question answering (QA) is a fundamental means to facilitate assessment and training of narrative comprehension skills for both machines and young children, yet there is scarcity of high-quality QA datasets carefully designed to serve this purpose. In particular, existing datasets rarely distinguish fine-grained reading skills, such as the understanding of varying narrative elements. Drawing on the reading education research, we introduce FairytaleQA, a dataset focusing on narrative comprehension of kindergarten to eighth-grade students. Generated by educational experts based on an evidence-based theoretical framework, FairytaleQA consists of 10,580 explicit and implicit questions derived from 278 children-friendly stories, covering seven types of narrative elements or relations. Our dataset is valuable in two folds: First, we ran existing QA models on our dataset and confirmed that this annotation helps assess models{'} fine-grained learning skills. Second, the dataset supports question generation (QG) task in the education domain. Through benchmarking with QG models, we show that the QG model trained on FairytaleQA is capable of asking high-quality and more diverse questions.",
}

@article{mou2022efficient,
  title={Efficient Long Sequence Encoding via Synchronization},
  author={Mou, Xiangyang and Yu, Mo and Yao, Bingsheng and Huang, Lifu},
  journal={arXiv preprint arXiv:2203.07644},
  year={2022}
}

@article{mou-etal-2021-narrative,
    title = "Narrative Question Answering with Cutting-Edge Open-Domain {QA} Techniques: A Comprehensive Study",
    author = "Mou, Xiangyang  and
      Yang, Chenghao  and
      Yu, Mo  and
      Yao, Bingsheng  and
      Guo, Xiaoxiao  and
      Potdar, Saloni  and
      Su, Hui",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.61",
    doi = "10.1162/tacl_a_00411",
    pages = "1032--1046",
    abstract = "Recent advancements in open-domain question answering (ODQA), that is, finding answers from large open-domain corpus like Wikipedia, have led to human-level performance on many datasets. However, progress in QA over book stories (Book QA) lags despite its similar task formulation to ODQA. This work provides a comprehensive and quantitative analysis about the difficulty of Book QA: (1) We benchmark the research on the NarrativeQA dataset with extensive experiments with cutting-edge ODQA techniques. This quantifies the challenges Book QA poses, as well as advances the published state-of-the-art with a ∼7{\%} absolute improvement on ROUGE-L. (2) We further analyze the detailed challenges in Book QA through human studies.1 Our findings indicate that the event-centric questions dominate this task, which exemplifies the inability of existing QA models to handle event-oriented scenarios.",
}

@inproceedings{yao-etal-2022-ais,
    title = "It is {AI}{'}s Turn to Ask Humans a Question: Question-Answer Pair Generation for Children{'}s Story Books",
    author = "Yao, Bingsheng  and
      Wang, Dakuo  and
      Wu, Tongshuang  and
      Zhang, Zheng  and
      Li, Toby  and
      Yu, Mo  and
      Xu, Ying",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.54",
    doi = "10.18653/v1/2022.acl-long.54",
    pages = "731--744",
    abstract = "Existing question answering (QA) techniques are created mainly to answer questions asked by humans. But in educational applications, teachers often need to decide what questions they should ask, in order to help students to improve their narrative understanding capabilities. We design an automated question-answer generation (QAG) system for this education scenario: given a story book at the kindergarten to eighth-grade level as input, our system can automatically generate QA pairs that are capable of testing a variety of dimensions of a student{'}s comprehension skills. Our proposed QAG model architecture is demonstrated using a new expert-annotated FairytaleQA dataset, which has 278 child-friendly storybooks with 10,580 QA pairs. Automatic and human evaluations show that our model outperforms state-of-the-art QAG baseline systems. On top of our QAG system, we also start to build an interactive story-telling application for the future real-world deployment in this educational scenario.",
}

@article{zhang2021building,
  title={Building a storytelling conversational agent through parent-ai collaboration},
  author={Zhang, Zheng and Xu, Ying and Wang, Yanhao and Wu, Tongshuang and Yao, Bingsheng and Ritchie, Daniel and Yu, Mo and Wang, Dakuo and Li, Toby Jia-Jun},
  year={2021}
}

@article{mou2020frustratingly,
  title={Frustratingly hard evidence retrieval for qa over books},
  author={Mou, Xiangyang and Yu, Mo and Yao, Bingsheng and Yang, Chenghao and Guo, Xiaoxiao and Potdar, Saloni and Su, Hui},
  journal={arXiv preprint arXiv:2007.09878},
  year={2020}
}

@inproceedings{10.1145/3377325.3377501,
author = {Drozdal, Jaimie and Weisz, Justin and Wang, Dakuo and Dass, Gaurav and Yao, Bingsheng and Zhao, Changruo and Muller, Michael and Ju, Lin and Su, Hui},
title = {Trust in AutoML: Exploring Information Needs for Establishing Trust in Automated Machine Learning Systems},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377501},
doi = {10.1145/3377325.3377501},
abstract = {We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {297–307},
numpages = {11},
keywords = {AutoML, automated data science, automated machine learning, AutoDS, automated artificial intelligence, AutoAI, trust},
location = {Cagliari, Italy},
series = {IUI '20}
}

